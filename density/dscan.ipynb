{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DBSCAN\n",
    "This notebook is split into three sections with one for every algorithm.\n",
    "Essentially, the idea is to cluster movies and users so that we can\n",
    "recommend the entire cluster they belong to, for both users and movies.\n",
    "\n",
    "Compared to the other algorithms, this means that we will not be able\n",
    "to give exactly how much they like a given movie, but just give them\n",
    "the cluster they reside in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Initialise PySpark and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "import pyspark.sql\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from sklearn.cluster import DBSCAN\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from density.slides_dbscan import my_DBSCAN\n",
    "\n",
    "if os.path.basename(os.getcwd()) == 'density':\n",
    "    print(\"Current dir is\", os.getcwd())\n",
    "    print(\"Changing dir to be in root\")\n",
    "    os.chdir('..')\n",
    "    print('now in', os.getcwd())\n",
    "\n",
    "from proposal.useful_tools import pickle_manager\n",
    "\n",
    "SPARK_CONF = SparkConf()\n",
    "SPARK_CONF.set(\"spark.driver.memory\", \"10g\")\n",
    "SPARK_CONF.set(\"spark.cores.max\", \"4\")\n",
    "SPARK_CONF.set(\"spark.executor.heartbeatInterval\", \"3600\")\n",
    "SPARK_CONF.setAppName(\"word2vec\")\n",
    "\n",
    "SPARK_CONTEXT = SparkContext.getOrCreate(SPARK_CONF)\n",
    "SPARK = SQLContext(SPARK_CONTEXT)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Assume that I can do this\n",
    "MOVIES: pd.DataFrame = pickle_manager.load_pickle(\n",
    "    'pickles/sentences.pickle.lz4'\n",
    ")\n",
    "print(MOVIES.head())\n",
    "\n",
    "MOVIES_FEATURES: pd.DataFrame = pickle_manager.load_pickle(\n",
    "    'pickles/sentence_features.pickle.lz4'\n",
    ")\n",
    "print(MOVIES_FEATURES.head())\n",
    "MOVIES_FEATURES = MOVIES_FEATURES.reset_index().to_numpy(dtype=np.float64)\n",
    "\n",
    "# Huh apparently we have nan. If we set to 0 then it should be fine\n",
    "MOVIES_FEATURES[np.isnan(MOVIES_FEATURES)] = 0\n",
    "print(MOVIES_FEATURES)\n",
    "\n",
    "pca_maker = PCA(n_components=2)\n",
    "PCA_DATA = pca_maker.fit_transform(MOVIES_FEATURES)\n",
    "print(PCA_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "PCA_DATA = list(zip(*PCA_DATA))\n",
    "PCA_x, PCA_y = PCA_DATA[0], PCA_DATA[1]\n",
    "print(PCA_DATA[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(PCA_x, PCA_y, s=0.005)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "So we actually have a pretty cool way to represent clusters visually by doing this! Ideally we can break\n",
    "this into chunks of movies with 100 or so recommendations. In theory, it should not just turn brown, but it\n",
    "might."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Finding Optimal Parameters\n",
    "To be safe with defining epsilon and radius, it was decided that the library\n",
    "will be used first. This can let us just train for the optimal\n",
    "epsilon and minimum samples while knowing it works.\n",
    "\n",
    "If you open a website for movies, you probably want to see at least 100 movies.\n",
    "Let's assume that's our minimum number of points then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# This is probably going to light up as copied from somewhere\n",
    "MIN_POINTS = 100\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "start = time.perf_counter()\n",
    "neighbors = NearestNeighbors(n_neighbors=MIN_POINTS, metric='cosine')\n",
    "neighbors_fit = neighbors.fit(MOVIES_FEATURES)\n",
    "distances, indices = neighbors_fit.kneighbors(MOVIES_FEATURES)\n",
    "print(\"That took\", time.perf_counter() - start)\n",
    "\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)\n",
    "plt.title('Number of nearest neighbours with a given distance between points')\n",
    "plt.xlabel('Number of points')\n",
    "plt.ylabel('Distance between points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our ideal value for epsilon is going to be sitting right at that turn in the centre. Most\n",
    "people just eye the value, but we can actually just use the second derivative to find this point.\n",
    "\n",
    "First issue is that this is not a continuous function, so we are going to have to find the\n",
    "discrete form of how to find the second derivative.\n",
    "\n",
    "Taking the derivative by first principle:\n",
    "$$\n",
    "    \\frac{dy}{dx} = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "Now to convert this to discrete values, we know that h does exist and cannot\n",
    "be infinitely close to 0 anymore. Instead, we are forced to select this distance\n",
    "by the space between the points, which should be 1 in our case.\n",
    "\n",
    "For our second derivative, we have to find the derivative of the values around the point\n",
    "then the derivative of those changes. So we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "maximum_curve = 0\n",
    "maximum_curve_index = 0\n",
    "\n",
    "# We need to use the point in front and behind so we need\n",
    "# to start at 1 and end one early\n",
    "for i in range(500, len(distances) - 1):\n",
    "    a_to_b = distances[i - 1] - distances[i]\n",
    "    b_to_c = distances[i + 1] - distances[i]\n",
    "    second_derivative = b_to_c - a_to_b\n",
    "    if i == 1:\n",
    "        maximum_curve = second_derivative\n",
    "        maximum_curve_index = 1\n",
    "\n",
    "    if second_derivative > maximum_curve:\n",
    "        maximum_curve = second_derivative\n",
    "        maximum_curve_index = i"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Maximum curvature occurs at\", maximum_curve_index)\n",
    "epsilon = distances[maximum_curve_index]\n",
    "print(\"That distance is\", epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that is a lot of possible inflections. Odds are that a lot of them are between 0 and 20,000 and 80,000\n",
    "for us though."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Scikit DBSCAN\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN)\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "scikit_movies_clustering = DBSCAN(\n",
    "    eps=epsilon, min_samples=MIN_POINTS, n_jobs=-1\n",
    ").fit(MOVIES_FEATURES)\n",
    "print(\"Took\", time.perf_counter() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def count_noise(labels):\n",
    "    count = 0\n",
    "    for i in labels:\n",
    "        if i == -1:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "print(scikit_movies_clustering.labels_)\n",
    "print(\"Unique labels\", set(scikit_movies_clustering.labels_))\n",
    "print(\"Number of noise points\", count_noise(scikit_movies_clustering.labels_))\n",
    "\n",
    "print(\"Frequency table\", np.unique(np.unique(scikit_movies_clustering.labels_, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "    print(\"==============================\")\n",
    "    print(\"Doing\", i)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    scikit_movies_clustering = DBSCAN(\n",
    "        eps=i, min_samples=5,  n_jobs=-1, metric='cosine'\n",
    "    ).fit(MOVIES_FEATURES)\n",
    "    print(\"Took\", time.perf_counter() - start, \"seconds\")\n",
    "\n",
    "    print(scikit_movies_clustering.labels_)\n",
    "    print(\"Unique labels\", set(scikit_movies_clustering.labels_))\n",
    "    print(\"Number of noise points\", count_noise(scikit_movies_clustering.labels_))\n",
    "\n",
    "    print(\"Frequency table\", np.unique(np.unique(scikit_movies_clustering.labels_, return_counts=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PyClustering DBSCAN\n",
    "### [Documentation](https://pyclustering.github.io/docs/0.10.1/html/d2/d42/classpyclustering_1_1cluster_1_1dbscan_1_1dbscan.html#details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pyclustering.cluster.dbscan import dbscan\n",
    "\n",
    "start = time.perf_counter()\n",
    "pyclustering_movies = dbscan(\n",
    "    MOVIES_SIMILARITY_MATRIX, MOVIES_RADIUS, MOVIES_MINIMUM_POINTS, True, **{'data_type': 'distance_matrix'}\n",
    ")\n",
    "pyclustering_movies.process()\n",
    "print(\"Took\", time.perf_counter() - start, \"seconds\")\n",
    "\n",
    "print(\"Clusters:\", pyclustering_movies.get_clusters())\n",
    "print(\"Noise:\", pyclustering_movies.get_noise())\n",
    "print(\"Number of noise points\", len(pyclustering_movies.get_noise()))\n",
    "print(\"Frequency table\", np.unique(np.unique(pyclustering_movies.get_clusters(), return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Slides\n",
    "### Implementation\n",
    "Since we made a class that just inherits the scikit dbscan and replaces the fit function, we should\n",
    "just be able to do the same process here as the scikit section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "my_dbscan_movies_clustering = my_DBSCAN(\n",
    "    eps=MOVIES_RADIUS, min_samples=MOVIES_MINIMUM_POINTS, metric='precomputed', n_jobs=-1\n",
    ").fit(MOVIES_SIMILARITY_MATRIX.copy())\n",
    "print(\"Took\", time.perf_counter() - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(set(my_dbscan_movies_clustering.labels_))\n",
    "print(\"Number of noise points\", count_noise(my_dbscan_movies_clustering.labels_))\n",
    "print(\"Frequency table\", np.unique(np.unique(my_dbscan_movies_clustering.labels_, return_counts=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def evaluate_clustering_numerically(labels, features):\n",
    "    \"\"\"\n",
    "    All of the prints that are needed to represent how well the data is clustered\n",
    "    :param labels:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"Number of clusters\", len(set(labels)) - (1 if -1 in labels else 0))\n",
    "    print(\"Noise fraction\", list(labels).count(-1) / len(labels))\n",
    "    print(\"Silhouette Coefficient\", metrics.silhouette_score(features, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "evaluate_clustering_numerically(\n",
    "    scikit_movies_clustering.labels_,\n",
    "    MOVIES_FEATURES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "evaluate_clustering_numerically(\n",
    "    pyclustering_movies.get_clusters(),\n",
    "    MOVIES_SIMILARITY_MATRIX\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "evaluate_clustering_numerically(\n",
    "    my_dbscan_movies_clustering.labels_,\n",
    "    MOVIES_SIMILARITY_MATRIX\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}