{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# DBSCAN\n",
    "This notebook is split into three sections with one for every algorithm.\n",
    "Essentially, the idea is to cluster movies and users so that we can\n",
    "recommend the entire cluster they belong to, for both users and movies.\n",
    "\n",
    "Compared to the other algorithms, this means that we will not be able\n",
    "to give exactly how much they like a given movie, but just give them\n",
    "the cluster they reside in."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialise PySpark and data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir is E:\\Users\\nicol\\Documents\\GitHub\\3804ICT-Data-mining-try-4\\density\n",
      "Changing dir to be in root\n",
      "now in E:\\Users\\nicol\\Documents\\GitHub\\3804ICT-Data-mining-try-4\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "import pyspark.sql\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from sklearn.cluster import DBSCAN\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from density.slides_dbscan import my_DBSCAN\n",
    "\n",
    "if os.path.basename(os.getcwd()) == 'density':\n",
    "    print(\"Current dir is\", os.getcwd())\n",
    "    print(\"Changing dir to be in root\")\n",
    "    os.chdir('..')\n",
    "    print('now in', os.getcwd())\n",
    "\n",
    "SPARK_CONF = SparkConf()\n",
    "SPARK_CONF.set(\"spark.driver.memory\", \"10g\")\n",
    "SPARK_CONF.set(\"spark.cores.max\", \"4\")\n",
    "SPARK_CONF.set(\"spark.executor.heartbeatInterval\", \"3600\")\n",
    "SPARK_CONF.setAppName(\"word2vec\")\n",
    "\n",
    "SPARK_CONTEXT = SparkContext.getOrCreate(SPARK_CONF)\n",
    "SPARK = SQLContext(SPARK_CONTEXT)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load data: TODO\n",
    "# Assume that I can do this\n",
    "MOVIES: pyspark.sql.DataFrame = 1  # data frame\n",
    "USERS: pyspark.sql.DataFrame = 1  # data frame\n",
    "\n",
    "# Just so that we have something lets just go ahead and do this\n",
    "size = 1000\n",
    "np.random.seed(42)\n",
    "MOVIES_SIMILARITY_MATRIX = np.random.rand(size, size)\n",
    "USERS_SIMILARITY_MATRIX = np.random.rand(size, size)\n",
    "\n",
    "# To keep things fair, initialise the necessary parameters right at the start\n",
    "MOVIES_RADIUS = 0.001\n",
    "MOVIES_MINIMUM_POINTS = 4\n",
    "\n",
    "USERS_RADIUS = 0.001\n",
    "USERS_MINIMUM_POINTS = 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37454012 0.95071431 0.73199394 ... 0.13681863 0.95023735 0.44600577]\n",
      " [0.18513293 0.54190095 0.87294584 ... 0.06895802 0.05705472 0.28218707]\n",
      " [0.26170568 0.2469788  0.90625458 ... 0.30978786 0.29004553 0.87141403]\n",
      " ...\n",
      " [0.08526213 0.09203751 0.69438213 ... 0.93032574 0.91330033 0.17538921]\n",
      " [0.76021961 0.96382646 0.00477478 ... 0.33470457 0.40011529 0.95614839]\n",
      " [0.11429115 0.15980354 0.82572709 ... 0.41807198 0.42867126 0.92944855]]\n"
     ]
    }
   ],
   "source": [
    "print(MOVIES_SIMILARITY_MATRIX)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scikit DBSCAN\n",
    "[Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN)\n",
    "### Implementation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "scikit_movies_clustering = DBSCAN(\n",
    "    eps=MOVIES_RADIUS, min_samples=MOVIES_MINIMUM_POINTS, metric='precomputed', n_jobs=-1\n",
    ").fit(MOVIES_SIMILARITY_MATRIX)\n",
    "\n",
    "scikit_users_clustering = DBSCAN(\n",
    "    eps=USERS_RADIUS, min_samples=USERS_MINIMUM_POINTS, metric='precomputed', n_jobs=-1\n",
    ").fit(MOVIES_SIMILARITY_MATRIX).fit(USERS_SIMILARITY_MATRIX)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, -1}\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, -1}\n"
     ]
    }
   ],
   "source": [
    "print(set(scikit_movies_clustering.labels_))\n",
    "print(set(scikit_users_clustering.labels_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PyCaret DBSCAN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Slides\n",
    "### Implementation\n",
    "Since we made a class that just inherits the scikit dbscan and replaces the fit function, we should\n",
    "just be able to do the same process here as the scikit section"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "my_dbscan_movies_clustering = my_DBSCAN(\n",
    "    eps=MOVIES_RADIUS, min_samples=MOVIES_MINIMUM_POINTS, metric='precomputed', n_jobs=-1\n",
    ").fit(MOVIES_SIMILARITY_MATRIX)\n",
    "\n",
    "my_dbscan_users_clustering = my_DBSCAN(\n",
    "    eps=USERS_RADIUS, min_samples=USERS_MINIMUM_POINTS, metric='precomputed', n_jobs=-1\n",
    ").fit(USERS_SIMILARITY_MATRIX)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0, 2.0, 3.0, -1.0}\n",
      "{0.0, 1.0, 2.0, -1.0}\n"
     ]
    }
   ],
   "source": [
    "print(set(my_dbscan_movies_clustering.labels_))\n",
    "print(set(my_dbscan_users_clustering.labels_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}